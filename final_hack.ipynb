{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMBE1uygIO6H",
        "outputId": "42a56e75-685e-42bf-a63d-7edc6bf26507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tolerance-based Test Accuracy (±2 units): 94.28%\n",
            "Mean Absolute Error: 0.6191928245514663\n",
            "Root Mean Squared Error: 1.1767689305984945\n",
            "External Test Accuracy (±2 units): 90.33%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"upstop_hachathon.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1HwGJ8IPu3jBVzinl12JCMX0412jhPIZ-\n",
        "\"\"\"\n",
        "\n",
        "'''import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Yield\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df[col] = encoders[col].fit_transform(df[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df[target] = label_encoder.fit_transform(df[target])\n",
        "\n",
        "# Split dataset (Before Applying SMOTE!)\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE only on training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Training Accuracy: {accuracy:.2%}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select specific training data\n",
        "df_subset = pd.concat([df.iloc[1:1000], df.iloc[1050:2000], df.iloc[2050:3000], df.iloc[3050:4000], df.iloc[4050:5000], df.iloc[5050:]]).copy()\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Yield\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df_subset[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features using Label Encoding (Save Encoders)\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df_subset[col] = encoders[col].fit_transform(df_subset[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "numeric_cols = df_subset.select_dtypes(include=[np.number]).columns\n",
        "df_subset[numeric_cols] = df_subset[numeric_cols].fillna(df_subset[numeric_cols].median())\n",
        "\n",
        "# Drop rows with missing target values\n",
        "df_subset = df_subset.dropna(subset=[target])\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df_subset[target] = label_encoder.fit_transform(df_subset[target])\n",
        "\n",
        "# Split dataset (Before Applying SMOTE!)\n",
        "X = df_subset[features]\n",
        "y = df_subset[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply SMOTE only on training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight=\"balanced\")\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "\n",
        "print(f\"Training Accuracy: {accuracy:.2%}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "### ---- Test on New Data (from Unused Ranges) ---- ###\n",
        "\n",
        "# Select test dataset from different index ranges\n",
        "df_check = pd.concat([df.iloc[1001:1049], df.iloc[2001:2049], df.iloc[3001:3049], df.iloc[4001:4049], df.iloc[5001:5049]]).copy()\n",
        "\n",
        "# Extract actual categories\n",
        "actual_labels = df_check[target].copy()\n",
        "\n",
        "# Apply the same LabelEncoders used during training\n",
        "for col in categorical_features:\n",
        "    df_check[col] = encoders[col].transform(df_check[col].astype(str))\n",
        "\n",
        "# Handle missing values in test set\n",
        "df_check.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_check[numeric_cols] = df_check[numeric_cols].fillna(df_check[numeric_cols].median())\n",
        "\n",
        "# Extract features\n",
        "df_check = df_check[features]\n",
        "\n",
        "# Predict categories\n",
        "predictions = rf_model.predict(df_check)\n",
        "\n",
        "# Convert predictions back to category labels\n",
        "predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "\n",
        "# Encode actual labels\n",
        "actual_labels_encoded = label_encoder.transform(actual_labels)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = (actual_labels_encoded == predictions).sum()\n",
        "total_predictions = len(actual_labels)\n",
        "\n",
        "print(f\"Total Predictions: {total_predictions}\")\n",
        "print(f\"Correct Predictions: {correct_predictions}\")\n",
        "print(f\"Test Accuracy: {correct_predictions / total_predictions:.2%}\")\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select specific training data\n",
        "# --- Training Data (~12,300 samples) ---\n",
        "df_subset = pd.concat([\n",
        "    df.iloc[:1000],\n",
        "    df.iloc[1050:2000],\n",
        "    df.iloc[2050:3000],\n",
        "    df.iloc[3050:4000],\n",
        "    df.iloc[4050:5000],\n",
        "    df.iloc[5050:6000],\n",
        "    df.iloc[6050:7000],\n",
        "    df.iloc[7050:8000],\n",
        "    df.iloc[8050:9000],\n",
        "    df.iloc[9050:10000],\n",
        "    df.iloc[10050:11000],\n",
        "    df.iloc[11050:12000],\n",
        "    df.iloc[12050:13000],\n",
        "]).copy()\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"   # <-- notice we now treat Yield as numeric, not class\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df_subset[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df_subset[col] = encoders[col].fit_transform(df_subset[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "numeric_cols = df_subset.select_dtypes(include=[np.number]).columns\n",
        "df_subset[numeric_cols] = df_subset[numeric_cols].fillna(df_subset[numeric_cols].median())\n",
        "\n",
        "# Split dataset\n",
        "X = df_subset[features]\n",
        "y = df_subset[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Regression Model ------------\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Accuracy within TOLERANCE range\n",
        "tolerance = 5  # can be adjusted\n",
        "within_range = np.abs(y_pred - y_test.values) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "\n",
        "# Optional: Regression Metrics\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "### ---- Test on New Unseen Data ---- ###\n",
        "\n",
        "# --- Testing Data (~3,100 samples) ---\n",
        "df_check = pd.concat([\n",
        "    df.iloc[1001:1049],\n",
        "    df.iloc[2001:2049],\n",
        "    df.iloc[3001:3049],\n",
        "    df.iloc[4001:4049],\n",
        "    df.iloc[5001:5049],\n",
        "    df.iloc[6001:6049],\n",
        "    df.iloc[7001:7049],\n",
        "    df.iloc[8001:8049],\n",
        "    df.iloc[9001:9049],\n",
        "    df.iloc[10001:10049],\n",
        "    df.iloc[11001:11049],\n",
        "    df.iloc[12001:12049],\n",
        "    df.iloc[13001:],  # This automatically collects data from 13001 to ~15400\n",
        "]).copy()\n",
        "actual_yield = df_check[target].copy()\n",
        "\n",
        "# Encode categorical features\n",
        "for col in categorical_features:\n",
        "    df_check[col] = encoders[col].transform(df_check[col].astype(str))\n",
        "\n",
        "# Handle missing values\n",
        "df_check.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_check[numeric_cols] = df_check[numeric_cols].fillna(df_check[numeric_cols].median())\n",
        "\n",
        "X_check = df_check[features]\n",
        "pred_yield = rf_model.predict(X_check)\n",
        "\n",
        "# Accuracy within tolerance for external test\n",
        "within_range_test = np.abs(pred_yield - actual_yield.values) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select specific training data\n",
        "# Approximation: 80% train, 20% test from 15,400 data points\n",
        "\n",
        "# --- Training Data (~12,300 samples) ---\n",
        "df_subset = pd.concat([\n",
        "    df.iloc[:1000],\n",
        "    df.iloc[1050:2000],\n",
        "    df.iloc[2050:3000],\n",
        "    df.iloc[3050:4000],\n",
        "    df.iloc[4050:5000],\n",
        "    df.iloc[5050:6000],\n",
        "    df.iloc[6050:7000],\n",
        "    df.iloc[7050:8000],\n",
        "    df.iloc[8050:9000],\n",
        "    df.iloc[9050:10000],\n",
        "    df.iloc[10050:11000],\n",
        "    df.iloc[11050:12000],\n",
        "    df.iloc[12050:13000],\n",
        "    df.iloc[13050:14000],\n",
        "    df.iloc[14050:15390],\n",
        "]).copy()\n",
        "\n",
        "# --- Testing Data (~3,100 samples) ---\n",
        "df_check = pd.concat([\n",
        "    df.iloc[1001:1049],\n",
        "    df.iloc[2001:2049],\n",
        "    df.iloc[3001:3049],\n",
        "    df.iloc[4001:4049],\n",
        "    df.iloc[5001:5049],\n",
        "    df.iloc[6001:6049],\n",
        "    df.iloc[7001:7049],\n",
        "    df.iloc[8001:8049],\n",
        "    df.iloc[9001:9049],\n",
        "    df.iloc[10001:10049],\n",
        "    df.iloc[11001:11049],\n",
        "    df.iloc[12001:12049],\n",
        "    df.iloc[13001:13049],\n",
        "    df.iloc[14001:14049],\n",
        "]).copy()\n",
        "\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"   # <-- notice we now treat Yield as numeric, not class\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df_subset[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df_subset[col] = encoders[col].fit_transform(df_subset[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "numeric_cols = df_subset.select_dtypes(include=[np.number]).columns\n",
        "df_subset[numeric_cols] = df_subset[numeric_cols].fillna(df_subset[numeric_cols].median())\n",
        "\n",
        "# Split dataset\n",
        "X = df_subset[features]\n",
        "y = df_subset[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Regression Model ------------\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Accuracy within TOLERANCE range\n",
        "tolerance = 2 # can be adjusted\n",
        "within_range = np.abs(y_pred - y_test.values) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "\n",
        "# Optional: Regression Metrics\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "### ---- Test on New Unseen Data ---- ###\n",
        "\n",
        "actual_yield = df_check[target].copy()\n",
        "\n",
        "# Encode categorical features\n",
        "for col in categorical_features:\n",
        "    df_check[col] = encoders[col].transform(df_check[col].astype(str))\n",
        "\n",
        "# Handle missing values\n",
        "df_check.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_check[numeric_cols] = df_check[numeric_cols].fillna(df_check[numeric_cols].median())\n",
        "\n",
        "X_check = df_check[features]\n",
        "pred_yield = rf_model.predict(X_check)\n",
        "\n",
        "# Accuracy within tolerance for external test\n",
        "within_range_test = np.abs(pred_yield - actual_yield.values) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select specific training data\n",
        "# Approximation: 80% train, 20% test from 15,400 data points\n",
        "\n",
        "# --- Training Data (~12,300 samples) ---\n",
        "df_subset = pd.concat([\n",
        "    df.iloc[:1000],\n",
        "    df.iloc[1050:2000],\n",
        "    df.iloc[2050:3000],\n",
        "    df.iloc[3050:4000],\n",
        "    df.iloc[4050:5000],\n",
        "    df.iloc[5050:6000],\n",
        "    df.iloc[6050:7000],\n",
        "    df.iloc[7050:8000],\n",
        "    df.iloc[8050:9000],\n",
        "    df.iloc[9050:10000],\n",
        "    df.iloc[10050:11000],\n",
        "    df.iloc[11050:12000],\n",
        "    df.iloc[12050:13000],\n",
        "    df.iloc[13050:14000],\n",
        "    df.iloc[14050:15390],\n",
        "]).copy()\n",
        "\n",
        "# --- Testing Data (~3,100 samples) ---\n",
        "df_check = pd.concat([\n",
        "    df.iloc[1001:1049],\n",
        "    df.iloc[2001:2049],\n",
        "    df.iloc[3001:3049],\n",
        "    df.iloc[4001:4049],\n",
        "    df.iloc[5001:5049],\n",
        "    df.iloc[6001:6049],\n",
        "    df.iloc[7001:7049],\n",
        "    df.iloc[8001:8049],\n",
        "    df.iloc[9001:9049],\n",
        "    df.iloc[10001:10049],\n",
        "    df.iloc[11001:11049],\n",
        "    df.iloc[12001:12049],\n",
        "    df.iloc[13001:13049],\n",
        "    df.iloc[14001:14049],\n",
        "]).copy()\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"   # <-- notice we now treat Yield as numeric, not class\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df_subset[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df_subset[col] = encoders[col].fit_transform(df_subset[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "numeric_cols = df_subset.select_dtypes(include=[np.number]).columns\n",
        "df_subset[numeric_cols] = df_subset[numeric_cols].fillna(df_subset[numeric_cols].median())\n",
        "\n",
        "# Split dataset\n",
        "X = df_subset[features]\n",
        "y = df_subset[target]\n",
        "\n",
        "# ----------- K-Fold Cross-Validation ------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rf_model = RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "\n",
        "cv_scores = cross_val_score(rf_model, X, y, cv=kf, scoring='neg_mean_absolute_error')\n",
        "print(f\"Cross-Validation MAE: {-np.mean(cv_scores):.4f}\")\n",
        "\n",
        "# Train the model on full training data\n",
        "rf_model.fit(X, y)\n",
        "\n",
        "# ----------- Testing on External Data ------------\n",
        "actual_yield = df_check[target].copy()\n",
        "\n",
        "# Encode categorical features\n",
        "for col in categorical_features:\n",
        "    df_check[col] = encoders[col].transform(df_check[col].astype(str))\n",
        "\n",
        "# Handle missing values\n",
        "df_check.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_check[numeric_cols] = df_check[numeric_cols].fillna(df_check[numeric_cols].median())\n",
        "\n",
        "X_check = df_check[features]\n",
        "pred_yield = rf_model.predict(X_check)\n",
        "\n",
        "# Accuracy within tolerance\n",
        "tolerance = 2  # can be adjusted\n",
        "within_range_test = np.abs(pred_yield - actual_yield.values) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(actual_yield, pred_yield))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(actual_yield, pred_yield)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDdrT19RomEv",
        "outputId": "46122ec1-79a7-4bc8-9de8-911c180bd0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation MAE: 0.6443\n",
            "External Test Accuracy (±2 units): 90.62%\n",
            "Mean Absolute Error: 0.8744401902339997\n",
            "Root Mean Squared Error: 1.596657957642737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from imblearn.over_sampling import KMeansSMOTE\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df[features].select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = df[features].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Preprocessing\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert categorical data using one-hot encoding\n",
        "X_train = pd.get_dummies(X_train, columns=categorical_features)\n",
        "X_test = pd.get_dummies(X_test, columns=categorical_features)\n",
        "\n",
        "# Align columns to ensure the same shape\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Apply KMeansSMOTE for regression\n",
        "smote = KMeansSMOTE(cluster_balance_threshold=0.1, random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Hyperparameter tuning for Random Forest\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 300],\n",
        "    \"max_depth\": [None, 20, 30],\n",
        "    \"min_samples_split\": [2, 5],\n",
        "    \"min_samples_leaf\": [1, 2]\n",
        "}\n",
        "\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Train additional models\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "xgb = XGBRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
        "gbr.fit(X_train_resampled, y_train_resampled)\n",
        "xgb.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Ensemble predictions\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "y_pred_gbr = gbr.predict(X_test)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "y_pred = (y_pred_rf + y_pred_gbr + y_pred_xgb) / 3\n",
        "\n",
        "# Accuracy within tolerance\n",
        "tolerance = 1\n",
        "within_range = np.abs(y_pred - y_test.values) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "aYSxyvD4R00P",
        "outputId": "436b920a-2607-400f-b216-5a1d59c5ce96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c012929ba1b6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Apply KMeansSMOTE for regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_balance_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mX_train_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Hyperparameter tuning for Random Forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;34m\"multilabel-sequences\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ]:\n\u001b[0;32m--> 222\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;34mf\"Unknown label type: {y_type}. Maybe you are trying to fit a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"classifier, which expects discrete classes on a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install smote-variants\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1wzvlsQCh20D",
        "outputId": "c168598b-5dc9-4f4a-d976-dd5ae5b7801c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting smote-variants\n",
            "  Downloading smote_variants-1.0.1-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from smote-variants) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from smote-variants) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from smote-variants) (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from smote-variants) (1.4.2)\n",
            "Collecting minisom (from smote-variants)\n",
            "  Downloading minisom-2.3.5.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting statistics (from smote-variants)\n",
            "  Downloading statistics-1.0.3.5.tar.gz (8.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from smote-variants) (2.18.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from smote-variants) (3.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from smote-variants) (2.2.2)\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from smote-variants) (2025.0.1)\n",
            "Collecting metric_learn (from smote-variants)\n",
            "  Downloading metric_learn-0.7.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from smote-variants) (0.13.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras->smote-variants) (24.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->smote-variants) (3.6.0)\n",
            "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->smote-variants) (2025.1.0)\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->smote-variants) (2022.1.0)\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->smote-variants) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->smote-variants) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->smote-variants) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->smote-variants) (2025.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn->smote-variants) (3.10.0)\n",
            "Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.11/dist-packages (from statistics->smote-variants) (0.21.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->smote-variants) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->smote-variants) (0.45.1)\n",
            "Requirement already satisfied: intel-cmplr-lib-ur==2025.1.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp>=2024->mkl->smote-variants) (2025.1.0)\n",
            "Requirement already satisfied: umf==0.10.* in /usr/local/lib/python3.11/dist-packages (from intel-cmplr-lib-ur==2025.1.0->intel-openmp>=2024->mkl->smote-variants) (0.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->smote-variants) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote-variants) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote-variants) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote-variants) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->smote-variants) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote-variants) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote-variants) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->smote-variants) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->smote-variants) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->smote-variants) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->smote-variants) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->smote-variants) (3.0.2)\n",
            "Downloading smote_variants-1.0.1-py3-none-any.whl (417 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.8/417.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading metric_learn-0.7.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: minisom, statistics\n",
            "  Building wheel for minisom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minisom: filename=MiniSom-2.3.5-py3-none-any.whl size=12031 sha256=c423f66b75874940b5de0466d31ece966e78ed98fa330279adbc425c202954c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/db/95/5e53bc2b88a328217fdf9f2886cafbe86b0df274f4b601f572\n",
            "  Building wheel for statistics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for statistics: filename=statistics-1.0.3.5-py3-none-any.whl size=7435 sha256=8d841d0f43fe4580e5c081f59de1b50a8b89c2a1c2ba899411c96dcc69757589\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/30/34/fceae1c718a4e749dd51f479c5720c0671519887e824915e90\n",
            "Successfully built minisom statistics\n",
            "Installing collected packages: minisom, statistics, metric_learn, smote-variants\n",
            "Successfully installed metric_learn-0.7.0 minisom-2.3.5 smote-variants-1.0.1 statistics-1.0.3.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "statistics"
                ]
              },
              "id": "d1c42788e9734e9c9602bba8d5f255f2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select specific training data\n",
        "df_subset = pd.concat([\n",
        "    df.iloc[:1000], df.iloc[1050:2000], df.iloc[2050:3000], df.iloc[3050:4000],\n",
        "    df.iloc[4050:5000], df.iloc[5050:6000], df.iloc[6050:7000], df.iloc[7050:8000],\n",
        "    df.iloc[8050:9000], df.iloc[9050:10000], df.iloc[10050:11000], df.iloc[11050:12000],\n",
        "    df.iloc[12050:13000], df.iloc[13050:14000], df.iloc[14050:15000]\n",
        "]).copy()\n",
        "\n",
        "df_check = pd.concat([\n",
        "    df.iloc[1001:1049], df.iloc[2001:2049], df.iloc[3001:3049], df.iloc[4001:4049],\n",
        "    df.iloc[5001:5049], df.iloc[6001:6049], df.iloc[7001:7049], df.iloc[8001:8049],\n",
        "    df.iloc[9001:9049], df.iloc[10001:10049], df.iloc[11001:11049], df.iloc[12001:12049],\n",
        "    df.iloc[13001:13049], df.iloc[14001:14049]\n",
        "]).copy()\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df_subset.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# One-Hot Encode categorical features\n",
        "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "encoded_features = encoder.fit_transform(df_subset[categorical_features])\n",
        "encoded_feature_names = encoder.get_feature_names_out(categorical_features)\n",
        "\n",
        "df_subset = df_subset.drop(columns=categorical_features)\n",
        "df_subset[encoded_feature_names] = encoded_features\n",
        "\n",
        "df_check[encoded_feature_names] = encoder.transform(df_check[categorical_features])\n",
        "df_check = df_check.drop(columns=categorical_features)\n",
        "\n",
        "# Handle missing values (fill with median for numeric columns)\n",
        "df_subset.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_check.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "numeric_cols = df_subset.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if target in numeric_cols:\n",
        "    numeric_cols.remove(target)\n",
        "\n",
        "df_subset[numeric_cols] = df_subset[numeric_cols].fillna(df_subset[numeric_cols].median())\n",
        "df_check[numeric_cols] = df_check[numeric_cols].fillna(df_check[numeric_cols].median())\n",
        "\n",
        "# Scaling features\n",
        "scaler = MinMaxScaler()\n",
        "df_subset[numeric_cols] = scaler.fit_transform(df_subset[numeric_cols])\n",
        "df_check[numeric_cols] = scaler.transform(df_check[numeric_cols])\n",
        "\n",
        "# Ensure `df_check` has the same columns as `df_subset`\n",
        "missing_cols = set(df_subset.columns) - set(df_check.columns)\n",
        "for col in missing_cols:\n",
        "    df_check[col] = 0  # Fill missing columns with zero\n",
        "\n",
        "df_check = df_check[df_subset.columns]  # Ensure correct order\n",
        "\n",
        "# Split dataset\n",
        "X = df_subset.drop(columns=[target])\n",
        "y = df_subset[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X_train = np.array(X_train).reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.array(X_test).reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, activation='relu', return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, activation='relu', return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "\n",
        "# Accuracy within tolerance range\n",
        "tolerance = 1\n",
        "within_range = np.abs(y_pred - y_test.values) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# --- Test on Unseen Data ---\n",
        "X_check = np.array(df_check.drop(columns=[target], errors='ignore')).reshape((df_check.shape[0], 1, X_check.shape[1]))\n",
        "actual_yield = df_check[target].copy() if target in df_check else None\n",
        "\n",
        "pred_yield = model.predict(X_check).flatten()\n",
        "\n",
        "within_range_test = np.abs(pred_yield - actual_yield.values) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPY9Q3gIM80v",
        "outputId": "37f59a3e-098d-4023-9ffc-b296afbb7d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - loss: 341.1368 - mae: 12.6890 - val_loss: 4.4893 - val_mae: 1.5546\n",
            "Epoch 2/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 12.6519 - mae: 2.5571 - val_loss: 2.8411 - val_mae: 1.2213\n",
            "Epoch 3/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 9.7672 - mae: 2.2177 - val_loss: 3.2464 - val_mae: 1.2015\n",
            "Epoch 4/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 9.5362 - mae: 2.1465 - val_loss: 2.6361 - val_mae: 1.0480\n",
            "Epoch 5/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 8.7724 - mae: 2.0373 - val_loss: 2.3909 - val_mae: 1.0739\n",
            "Epoch 6/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 8.3198 - mae: 1.9988 - val_loss: 3.1880 - val_mae: 1.3461\n",
            "Epoch 7/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 8.0194 - mae: 1.9464 - val_loss: 2.1034 - val_mae: 1.0084\n",
            "Epoch 8/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.9442 - mae: 1.9274 - val_loss: 2.2850 - val_mae: 1.0063\n",
            "Epoch 9/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 7.4356 - mae: 1.8614 - val_loss: 1.5058 - val_mae: 0.7810\n",
            "Epoch 10/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.3979 - mae: 1.8351 - val_loss: 2.1338 - val_mae: 0.9465\n",
            "Epoch 11/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 7.1175 - mae: 1.7868 - val_loss: 2.6314 - val_mae: 1.1744\n",
            "Epoch 12/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 6.4785 - mae: 1.6916 - val_loss: 3.2172 - val_mae: 1.1919\n",
            "Epoch 13/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5229 - mae: 1.5829 - val_loss: 9.5211 - val_mae: 2.3530\n",
            "Epoch 14/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.9620 - mae: 1.5027 - val_loss: 5.8100 - val_mae: 1.8423\n",
            "Epoch 15/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.9607 - mae: 1.4666 - val_loss: 5.6007 - val_mae: 1.6367\n",
            "Epoch 16/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.4162 - mae: 1.3863 - val_loss: 6.5095 - val_mae: 1.8432\n",
            "Epoch 17/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 4.0810 - mae: 1.3371 - val_loss: 10.1306 - val_mae: 2.3429\n",
            "Epoch 18/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.0816 - mae: 1.2996 - val_loss: 11.2806 - val_mae: 2.5192\n",
            "Epoch 19/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.8516 - mae: 1.2669 - val_loss: 18.6804 - val_mae: 3.2724\n",
            "Epoch 20/20\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.4979 - mae: 1.2032 - val_loss: 16.0225 - val_mae: 2.9954\n",
            "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
            "Tolerance-based Test Accuracy (±1 units): 25.42%\n",
            "Mean Absolute Error: 2.9954067895655396\n",
            "Root Mean Squared Error: 4.002810440588302\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 32928 into shape (672,1,1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-483f7c63bf0b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# --- Test on Unseen Data ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mX_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mactual_yield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_check\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_check\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 32928 into shape (672,1,1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = df[features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df[col] = encoders[col].fit_transform(df[col].astype(str))\n",
        "\n",
        "# Handle missing or infinite values\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(df.median(), inplace=True)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Split dataset into train and test\n",
        "X = df[features].values  # Convert to NumPy array\n",
        "y = df[target].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for LSTM: (samples, time steps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy within tolerance\n",
        "tolerance = 1  # Can be adjusted\n",
        "within_range = np.abs(y_pred.flatten() - y_test) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# --- Test on Unseen Data ---\n",
        "df_check = df.sample(n=500, random_state=42)  # Sample unseen data\n",
        "\n",
        "X_check = df_check[features].values\n",
        "actual_yield = df_check[target].values\n",
        "\n",
        "# Reshape for LSTM\n",
        "X_check = X_check.reshape((X_check.shape[0], 1, X_check.shape[1]))\n",
        "\n",
        "pred_yield = model.predict(X_check)\n",
        "\n",
        "# Accuracy within tolerance for external test\n",
        "within_range_test = np.abs(pred_yield.flatten() - actual_yield) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "T3pg5Q3TY95s",
        "outputId": "15f6b070-d338-4e1c-fbf8-825e80d1f9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot convert [['low_acidic' 'low_acidic' 'low_acidic' ... 'low_acidic' 'low_acidic'\n  'low_acidic']] to numeric",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f9e4709e3bdb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Handle missing or infinite values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Normalize numerical features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11704\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11705\u001b[0m     ):\n\u001b[0;32m> 11706\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11708\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"median\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12429\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12430\u001b[0m     ) -> Series | float:\n\u001b[0;32m> 12431\u001b[0;31m         return self._stat_function(\n\u001b[0m\u001b[1;32m  12432\u001b[0m             \u001b[0;34m\"median\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmedian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12433\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m         \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"skipna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_allowed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12377\u001b[0;31m         return self._reduce(\n\u001b[0m\u001b[1;32m  12378\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  12379\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m  11560\u001b[0m         \u001b[0;31m# After possibly _get_data and transposing, we are now in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11561\u001b[0m         \u001b[0;31m#  simple case where we can use BlockManager.reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11562\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11563\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"boolean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0mres_blocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m             \u001b[0mnbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m             \u001b[0mres_blocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mblk_func\u001b[0;34m(values, axis)\u001b[0m\n\u001b[1;32m  11479\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11480\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11481\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11483\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    145\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmedian\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot convert {values} to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert [['low_acidic' 'low_acidic' 'low_acidic' ... 'low_acidic' 'low_acidic'\n  'low_acidic']] to numeric"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"soil.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define features and target\n",
        "features = [\"Name\", \"Fertility\", \"Photoperiod\", \"Temperature\", \"Rainfall\", \"pH\", \"Light_Hours\",\n",
        "            \"Light_Intensity\", \"Rh\", \"Nitrogen\", \"Phosphorus\", \"Potassium\", \"Soil_Type\", \"Season\"]\n",
        "target = \"Yield\"\n",
        "\n",
        "# Identify categorical and numeric features separately\n",
        "categorical_features = df[features].select_dtypes(include=['object']).columns.tolist()\n",
        "numeric_features = df[features].select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "# Encode categorical features\n",
        "encoders = {col: LabelEncoder() for col in categorical_features}\n",
        "for col in categorical_features:\n",
        "    df[col] = encoders[col].fit_transform(df[col].astype(str))\n",
        "\n",
        "# Handle missing values correctly\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df[numeric_features] = df[numeric_features].fillna(df[numeric_features].median())  # Apply only to numeric columns\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Split dataset into train and test\n",
        "X = df[features].values  # Convert to NumPy array\n",
        "y = df[target].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape for LSTM: (samples, time steps, features)\n",
        "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(128, return_sequences=True, input_shape=(1, X_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy within tolerance\n",
        "tolerance = 1  # Can be adjusted\n",
        "within_range = np.abs(y_pred.flatten() - y_test) <= tolerance\n",
        "range_accuracy = within_range.sum() / len(y_test)\n",
        "\n",
        "print(f\"Tolerance-based Test Accuracy (±{tolerance} units): {range_accuracy:.2%}\")\n",
        "print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# --- Test on Unseen Data ---\n",
        "df_check = df.sample(n=500, random_state=42)  # Sample unseen data\n",
        "\n",
        "X_check = df_check[features].values\n",
        "actual_yield = df_check[target].values\n",
        "\n",
        "# Reshape for LSTM\n",
        "X_check = X_check.reshape((X_check.shape[0], 1, X_check.shape[1]))\n",
        "\n",
        "pred_yield = model.predict(X_check)\n",
        "\n",
        "# Accuracy within tolerance for external test\n",
        "within_range_test = np.abs(pred_yield.flatten() - actual_yield) <= tolerance\n",
        "range_accuracy_test = within_range_test.sum() / len(actual_yield)\n",
        "\n",
        "print(f\"External Test Accuracy (±{tolerance} units): {range_accuracy_test:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFS7UlC7ZSvS",
        "outputId": "db11850d-3da5-4deb-f0d9-e3120491e2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 384.1764 - mae: 14.4013 - val_loss: 56.0594 - val_mae: 6.2331\n",
            "Epoch 2/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 49.6901 - mae: 5.7537 - val_loss: 26.2129 - val_mae: 4.1765\n",
            "Epoch 3/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 29.9968 - mae: 4.4567 - val_loss: 17.2152 - val_mae: 3.3365\n",
            "Epoch 4/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 21.9590 - mae: 3.7567 - val_loss: 13.1013 - val_mae: 2.9162\n",
            "Epoch 5/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 17.4507 - mae: 3.3057 - val_loss: 10.2700 - val_mae: 2.5029\n",
            "Epoch 6/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 14.6132 - mae: 2.9635 - val_loss: 8.6671 - val_mae: 2.2733\n",
            "Epoch 7/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 12.9696 - mae: 2.7726 - val_loss: 7.5234 - val_mae: 2.0861\n",
            "Epoch 8/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 11.3344 - mae: 2.5648 - val_loss: 6.3499 - val_mae: 1.9015\n",
            "Epoch 9/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 10.5620 - mae: 2.4484 - val_loss: 5.7371 - val_mae: 1.7654\n",
            "Epoch 10/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 9.9898 - mae: 2.3157 - val_loss: 5.8167 - val_mae: 1.7477\n",
            "Epoch 11/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 9.6899 - mae: 2.2633 - val_loss: 5.0181 - val_mae: 1.6014\n",
            "Epoch 12/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.6904 - mae: 2.1225 - val_loss: 4.7148 - val_mae: 1.5524\n",
            "Epoch 13/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 8.5778 - mae: 2.0942 - val_loss: 4.4220 - val_mae: 1.4789\n",
            "Epoch 14/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 8.0874 - mae: 2.0314 - val_loss: 4.4806 - val_mae: 1.4863\n",
            "Epoch 15/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.8019 - mae: 1.9833 - val_loss: 4.9375 - val_mae: 1.5690\n",
            "Epoch 16/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.6298 - mae: 1.9337 - val_loss: 4.0651 - val_mae: 1.4105\n",
            "Epoch 17/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.0552 - mae: 1.8637 - val_loss: 3.8155 - val_mae: 1.3541\n",
            "Epoch 18/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 7.1461 - mae: 1.8700 - val_loss: 3.5042 - val_mae: 1.3096\n",
            "Epoch 19/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 6.8296 - mae: 1.8242 - val_loss: 3.5167 - val_mae: 1.2870\n",
            "Epoch 20/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 6.0728 - mae: 1.7300 - val_loss: 3.2820 - val_mae: 1.2423\n",
            "Epoch 21/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 6.0726 - mae: 1.7064 - val_loss: 3.5500 - val_mae: 1.2932\n",
            "Epoch 22/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.8952 - mae: 1.6782 - val_loss: 3.0193 - val_mae: 1.1897\n",
            "Epoch 23/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.3906 - mae: 1.6055 - val_loss: 2.8457 - val_mae: 1.1485\n",
            "Epoch 24/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 5.5722 - mae: 1.5992 - val_loss: 2.9072 - val_mae: 1.1575\n",
            "Epoch 25/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.4934 - mae: 1.5857 - val_loss: 2.6246 - val_mae: 1.0996\n",
            "Epoch 26/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 5.2636 - mae: 1.5562 - val_loss: 3.2962 - val_mae: 1.2739\n",
            "Epoch 27/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 5.0028 - mae: 1.5173 - val_loss: 2.8632 - val_mae: 1.1344\n",
            "Epoch 28/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.8119 - mae: 1.4848 - val_loss: 2.6199 - val_mae: 1.0817\n",
            "Epoch 29/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.7326 - mae: 1.4862 - val_loss: 2.4496 - val_mae: 1.0492\n",
            "Epoch 30/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.5055 - mae: 1.4272 - val_loss: 2.3441 - val_mae: 1.0164\n",
            "Epoch 31/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.4000 - mae: 1.4246 - val_loss: 2.3309 - val_mae: 1.0092\n",
            "Epoch 32/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.3261 - mae: 1.3961 - val_loss: 2.2120 - val_mae: 0.9722\n",
            "Epoch 33/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 4.2154 - mae: 1.3737 - val_loss: 2.2919 - val_mae: 1.0092\n",
            "Epoch 34/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 3.9840 - mae: 1.3436 - val_loss: 2.1853 - val_mae: 0.9656\n",
            "Epoch 35/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.8978 - mae: 1.3210 - val_loss: 2.1914 - val_mae: 0.9783\n",
            "Epoch 36/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 4.1295 - mae: 1.3413 - val_loss: 1.9879 - val_mae: 0.9065\n",
            "Epoch 37/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.7920 - mae: 1.2878 - val_loss: 2.0858 - val_mae: 0.9224\n",
            "Epoch 38/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 3.5438 - mae: 1.2456 - val_loss: 1.9933 - val_mae: 0.9018\n",
            "Epoch 39/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 3.7456 - mae: 1.2537 - val_loss: 2.2847 - val_mae: 0.9508\n",
            "Epoch 40/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.5618 - mae: 1.2471 - val_loss: 2.0570 - val_mae: 0.9329\n",
            "Epoch 41/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.3864 - mae: 1.2040 - val_loss: 2.0384 - val_mae: 0.9160\n",
            "Epoch 42/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3.2590 - mae: 1.2004 - val_loss: 1.8214 - val_mae: 0.8526\n",
            "Epoch 43/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 3.1811 - mae: 1.1658 - val_loss: 1.8304 - val_mae: 0.8521\n",
            "Epoch 44/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 3.2361 - mae: 1.1700 - val_loss: 1.8490 - val_mae: 0.8402\n",
            "Epoch 45/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 3.0615 - mae: 1.1519 - val_loss: 1.9013 - val_mae: 0.8750\n",
            "Epoch 46/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.0519 - mae: 1.1311 - val_loss: 1.9158 - val_mae: 0.8516\n",
            "Epoch 47/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - loss: 3.0366 - mae: 1.1188 - val_loss: 1.7109 - val_mae: 0.8047\n",
            "Epoch 48/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.7328 - mae: 1.0786 - val_loss: 2.1647 - val_mae: 0.9220\n",
            "Epoch 49/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.9266 - mae: 1.1062 - val_loss: 1.7804 - val_mae: 0.8472\n",
            "Epoch 50/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.7562 - mae: 1.0762 - val_loss: 1.6824 - val_mae: 0.7984\n",
            "Epoch 51/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.8332 - mae: 1.0805 - val_loss: 1.9117 - val_mae: 0.8576\n",
            "Epoch 52/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.9784 - mae: 1.1000 - val_loss: 1.7280 - val_mae: 0.8090\n",
            "Epoch 53/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.8645 - mae: 1.0683 - val_loss: 1.7817 - val_mae: 0.8327\n",
            "Epoch 54/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.6680 - mae: 1.0453 - val_loss: 1.6862 - val_mae: 0.8021\n",
            "Epoch 55/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.7554 - mae: 1.0479 - val_loss: 1.6448 - val_mae: 0.7864\n",
            "Epoch 56/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.6537 - mae: 1.0290 - val_loss: 1.6915 - val_mae: 0.7922\n",
            "Epoch 57/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.6455 - mae: 1.0387 - val_loss: 1.5702 - val_mae: 0.7547\n",
            "Epoch 58/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.4210 - mae: 0.9872 - val_loss: 1.5833 - val_mae: 0.7565\n",
            "Epoch 59/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.5262 - mae: 1.0083 - val_loss: 1.6641 - val_mae: 0.7652\n",
            "Epoch 60/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.6590 - mae: 1.0113 - val_loss: 1.7393 - val_mae: 0.7985\n",
            "Epoch 61/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.4266 - mae: 0.9704 - val_loss: 1.8088 - val_mae: 0.8347\n",
            "Epoch 62/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2.4536 - mae: 0.9857 - val_loss: 1.6933 - val_mae: 0.7832\n",
            "Epoch 63/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.4705 - mae: 0.9770 - val_loss: 1.5845 - val_mae: 0.7582\n",
            "Epoch 64/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3398 - mae: 0.9474 - val_loss: 1.6268 - val_mae: 0.7706\n",
            "Epoch 65/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2.4872 - mae: 0.9741 - val_loss: 1.6527 - val_mae: 0.7625\n",
            "Epoch 66/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3002 - mae: 0.9285 - val_loss: 1.5460 - val_mae: 0.7413\n",
            "Epoch 67/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.3766 - mae: 0.9578 - val_loss: 1.5563 - val_mae: 0.7335\n",
            "Epoch 68/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2226 - mae: 0.9282 - val_loss: 1.5114 - val_mae: 0.7099\n",
            "Epoch 69/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.3612 - mae: 0.9426 - val_loss: 1.5149 - val_mae: 0.7200\n",
            "Epoch 70/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - loss: 2.3375 - mae: 0.9295 - val_loss: 1.5435 - val_mae: 0.7190\n",
            "Epoch 71/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2.1284 - mae: 0.8960 - val_loss: 1.5599 - val_mae: 0.7213\n",
            "Epoch 72/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 2.2473 - mae: 0.9078 - val_loss: 1.5938 - val_mae: 0.7157\n",
            "Epoch 73/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 2.2201 - mae: 0.8937 - val_loss: 1.5142 - val_mae: 0.7148\n",
            "Epoch 74/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 2.3061 - mae: 0.9154 - val_loss: 1.7973 - val_mae: 0.8415\n",
            "Epoch 75/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2074 - mae: 0.8980 - val_loss: 1.4817 - val_mae: 0.6936\n",
            "Epoch 76/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.1992 - mae: 0.8896 - val_loss: 1.6303 - val_mae: 0.7436\n",
            "Epoch 77/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2.3007 - mae: 0.9080 - val_loss: 1.5444 - val_mae: 0.7149\n",
            "Epoch 78/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.0566 - mae: 0.8764 - val_loss: 1.7557 - val_mae: 0.7593\n",
            "Epoch 79/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1492 - mae: 0.8806 - val_loss: 1.4842 - val_mae: 0.7014\n",
            "Epoch 80/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2.2301 - mae: 0.8858 - val_loss: 1.5106 - val_mae: 0.7186\n",
            "Epoch 81/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 2.0662 - mae: 0.8624 - val_loss: 1.4656 - val_mae: 0.6869\n",
            "Epoch 82/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1148 - mae: 0.8680 - val_loss: 1.5492 - val_mae: 0.7123\n",
            "Epoch 83/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0985 - mae: 0.8732 - val_loss: 1.5907 - val_mae: 0.7316\n",
            "Epoch 84/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.1025 - mae: 0.8670 - val_loss: 1.4977 - val_mae: 0.6855\n",
            "Epoch 85/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.1941 - mae: 0.8842 - val_loss: 1.7958 - val_mae: 0.7547\n",
            "Epoch 86/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.3097 - mae: 0.8850 - val_loss: 1.5146 - val_mae: 0.7051\n",
            "Epoch 87/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1042 - mae: 0.8605 - val_loss: 1.4666 - val_mae: 0.6818\n",
            "Epoch 88/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2.1560 - mae: 0.8731 - val_loss: 1.4795 - val_mae: 0.6844\n",
            "Epoch 89/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 2.1475 - mae: 0.8619 - val_loss: 1.5158 - val_mae: 0.7176\n",
            "Epoch 90/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1398 - mae: 0.8725 - val_loss: 1.4895 - val_mae: 0.7050\n",
            "Epoch 91/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2720 - mae: 0.8803 - val_loss: 1.4986 - val_mae: 0.6955\n",
            "Epoch 92/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.1421 - mae: 0.8599 - val_loss: 1.5828 - val_mae: 0.7038\n",
            "Epoch 93/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - loss: 2.1595 - mae: 0.8590 - val_loss: 1.5570 - val_mae: 0.7229\n",
            "Epoch 94/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.0881 - mae: 0.8618 - val_loss: 1.4585 - val_mae: 0.6734\n",
            "Epoch 95/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.3083 - mae: 0.8827 - val_loss: 1.4648 - val_mae: 0.6831\n",
            "Epoch 96/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.0310 - mae: 0.8347 - val_loss: 1.5069 - val_mae: 0.7008\n",
            "Epoch 97/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.1364 - mae: 0.8605 - val_loss: 1.5024 - val_mae: 0.7070\n",
            "Epoch 98/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 2.0504 - mae: 0.8470 - val_loss: 1.5238 - val_mae: 0.6948\n",
            "Epoch 99/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.1726 - mae: 0.8516 - val_loss: 1.4917 - val_mae: 0.7048\n",
            "Epoch 100/100\n",
            "\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 2.0453 - mae: 0.8432 - val_loss: 1.5047 - val_mae: 0.6981\n",
            "\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Tolerance-based Test Accuracy (±1 units): 80.32%\n",
            "Mean Absolute Error: 0.6981092194418878\n",
            "Root Mean Squared Error: 1.2266632137366458\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
            "External Test Accuracy (±1 units): 80.60%\n"
          ]
        }
      ]
    }
  ]
}